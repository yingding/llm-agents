{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "this notebook demos example of using llm in a MPS backend (apple silicon GPU) using torch 2.x\n",
    "\n",
    "Referece:\n",
    "* torch 2.x MPS Backend: https://pytorch.org/docs/stable/notes/mps.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0.6\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import applyllm as apl\n",
    "\n",
    "print(apl.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS is available\n",
      "mps\n"
     ]
    }
   ],
   "source": [
    "# check that MPS is availabe (Metal Performance Shaders)\n",
    "if not torch.backends.mps.is_available():\n",
    "    print(\"MPS is not available\")\n",
    "else:\n",
    "    print(\"MPS is available\")\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    print(mps_device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/yingding/MODELS\n"
     ]
    }
   ],
   "source": [
    "from applyllm.accelerators import (\n",
    "    DirectorySetting,\n",
    "    TokenHelper as th,\n",
    ")\n",
    "    \n",
    "dir_mode_map = {\n",
    "    \"kf_notebook\": DirectorySetting(),\n",
    "    \"mac_local\": DirectorySetting(home_dir=\"/Users/yingding\", transformers_cache_home=\"MODELS\", huggingface_token_file=\"MODELS/.huggingface_token\"),\n",
    "}\n",
    "\n",
    "model_map = {\n",
    "    \"llama7B-chat\":     \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    \"llama13B-chat\" :   \"meta-llama/Llama-2-13b-chat-hf\",\n",
    "    \"llama70B-chat\" :   \"meta-llama/Llama-2-70b-chat-hf\",\n",
    "    \"mistral7B-01\":     \"mistralai/Mistral-7B-v0.1\",\n",
    "    \"mistral7B-inst02\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    \"mixtral8x7B-01\":   \"mistralai/Mixtral-8x7B-v0.1\",\n",
    "    \"mixtral8x7B-inst01\":   \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "    \"gemma7b-it\": \"google/gemma-7b-it\",\n",
    "    \"gemma7b\" : \"google/gemma-7b\",\n",
    "    \"gemma2b-it\": \"google/gemma-2b-it\",\n",
    "    \"gemma2b\" : \"google/gemma-2b\",\n",
    "}\n",
    "\n",
    "default_model_type = \"mistral7B-01\"\n",
    "default_dir_mode = \"mac_local\"\n",
    "\n",
    "dir_setting = dir_mode_map[default_dir_mode]\n",
    "\n",
    "os.environ[\"WORLD_SIZE\"] = \"1\" \n",
    "os.environ['XDG_CACHE_HOME'] = dir_setting.get_cache_home()\n",
    "\n",
    "print(os.environ['XDG_CACHE_HOME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.39.3\n",
      "2.2.2\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "print(transformers.__version__)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "google/gemma-2b-it\n"
     ]
    }
   ],
   "source": [
    "# model_type = default_model_type\n",
    "# model_type = \"gemma7b-it\"\n",
    "model_type = \"gemma2b-it\"\n",
    "# model_type = \"mistral7B-inst02\"\n",
    "# model_type = \"llama7B-chat\"\n",
    "# model_type = \"llama13B-chat\"\n",
    "\n",
    "model_name = model_map.get(model_type, default_model_type)\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fast tokenizer\n",
    "\n",
    "* https://github.com/huggingface/transformers/issues/23889#issuecomment-1584090357"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load LLM Model and then Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface token is NOT needed\n",
      "token_kwargs: {}\n",
      "model_kwargs: {'torch_dtype': torch.float16, 'device_map': 'auto', 'max_length': None}\n",
      "pipeline_kwargs: {'torch_dtype': torch.float16, 'device_map': 'auto', 'max_length': None, 'task': 'text-generation', 'max_new_tokens': 200, 'do_sample': True, 'temperature': 0.001, 'top_k': 3, 'top_p': 0.95, 'framework': 'pt', 'return_full_text': False, 'add_special_tokens': True}\n"
     ]
    }
   ],
   "source": [
    "from applyllm.pipelines import (\n",
    "    ModelCatalog,\n",
    "    KwargsBuilder\n",
    ")\n",
    "token_kwargs = th.gen_token_kwargs(model_type=model_type, dir_setting=dir_setting)\n",
    "print(f\"token_kwargs: {token_kwargs}\")\n",
    "\n",
    "# data_type = torch.bfloat16\n",
    "data_type = torch.float16\n",
    "device_map = \"auto\" # \"mps\" # \"auto\"  \n",
    "# auto caste not working for mps 4.38.2\n",
    "# https://github.com/huggingface/transformers/issues/29431 \n",
    "\n",
    "# mixtral model has no max_new_tokens limit, so it is not set here.\n",
    "model_kwargs = {\n",
    "    \"torch_dtype\": data_type, #bfloat16 is not supported on MPS backend, float16 only on GPU accelerator\n",
    "    # torch_dtype=torch.float32,\n",
    "    # max_length=MAX_LENGTH,\n",
    "    \"device_map\": device_map,\n",
    "    \"max_length\" : None, # remove the total length of the generated response\n",
    "}\n",
    "print(f\"model_kwargs: {model_kwargs}\")\n",
    "\n",
    "# set the transformers.pipeline kwargs\n",
    "# the torch_dtype shall be set both for the model and the pipeline, due to a transformer issue.\n",
    "# otherwise it will cause unnecessary more memory usage in the pipeline of transformers\n",
    "# https://github.com/huggingface/transformers/issues/28817\n",
    "# https://github.com/mlflow/mlflow/pull/10979\n",
    "\n",
    "# Set transformers.pipeline only to return generated text return_full_text=False\n",
    "# https://github.com/huggingface/transformers/issues/17117#issuecomment-1120809167\n",
    "pipeline_kwargs = {\n",
    "    \"task\": \"text-generation\",\n",
    "    \"max_new_tokens\" : 200,\n",
    "    \"do_sample\" : True, # do_sample True is required for temperature\n",
    "    \"temperature\" : 0.001, \n",
    "    \"device_map\" : device_map, # use the MPS device if available\n",
    "    \"top_k\": 3,\n",
    "    \"top_p\": 0.95,\n",
    "    # \"num_return_sequences\": 1,\n",
    "    \"framework\": \"pt\", # use pytorch as framework\n",
    "    \"return_full_text\": False, # return only the generated text, not the input text with the generated text\n",
    "}\n",
    "\n",
    "gemma_pipeline_kwargs = {\n",
    "    \"add_special_tokens\": True,\n",
    "    \"torch_dtype\": data_type,\n",
    "}\n",
    "\n",
    "# pipeline_kwargs override the model_kwargs during the merge\n",
    "pipeline_kwargs = KwargsBuilder([model_kwargs]).override(pipeline_kwargs).build()\n",
    "\n",
    "if model_name.startswith(ModelCatalog.GOOGLE_FAMILY):\n",
    "    pipeline_kwargs = KwargsBuilder([pipeline_kwargs]).override(gemma_pipeline_kwargs).build()\n",
    "\n",
    "print(f\"pipeline_kwargs: {pipeline_kwargs}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max memory to offload parts of LLM model to the CPU memory\n",
    "* https://huggingface.co/docs/accelerate/concept_guides/big_model_inference#designing-a-device-map\n",
    "\n",
    "Note:\n",
    "* Max Memory offload to CPU is CUDA implementation only\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gemma's activation function should be approximate GeLU and not exact GeLU.\n",
      "Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a16fd15e35654077ba12df7ba6c32770",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "executed: load_model() python function\n",
      "walltime: 2.964432954788208 in secs.\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from applyllm.utils import time_func\n",
    "from applyllm.pipelines import ModelConfig, LocalCausalLMConfig\n",
    "\n",
    "\n",
    "base_lm_config = ModelConfig(\n",
    "  model_config = {\n",
    "    \"pretrained_model_name_or_path\": model_name,\n",
    "    \"device_map\": device_map,\n",
    "  }\n",
    ")\n",
    "\n",
    "# No bitsandbytes qunatization support for MPS backend yet, set quantized to False\n",
    "kwargs = {\n",
    "  \"quantized\": False,\n",
    "  \"model_config\": base_lm_config.get_config(),\n",
    "  \"quantization_config\": {\n",
    "    \"quantization_config\": transformers.BitsAndBytesConfig(\n",
    "      load_in_4bit=True,\n",
    "      bnb_4bit_quant_type='nf4',\n",
    "      bnb_4bit_use_double_quant=True,\n",
    "      bnb_4bit_compute_dtype=torch.bfloat16\n",
    "      )\n",
    "    }\n",
    "}\n",
    "\n",
    "lm_config = LocalCausalLMConfig(**kwargs)\n",
    "\n",
    "@time_func\n",
    "def load_model():\n",
    "  return AutoModelForCausalLM.from_pretrained(    \n",
    "    **lm_config.get_config(),\n",
    "    **token_kwargs,  \n",
    "  )\n",
    "\n",
    "model = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_kwargs = {\n",
    "    \"model_config\": {\n",
    "        \"pretrained_model_name_or_path\": model_name,\n",
    "        \"device\": \"cpu\",\n",
    "        # \"device_map\": \"auto\", # put to GPU if GPU is available\n",
    "        # \"max_position_embeddings\": MAX_LENGTH,\n",
    "        # \"max_length\": MAX_LENGTH,\n",
    "    },\n",
    "}\n",
    "tokenizer_config = ModelConfig(**tokenizer_kwargs)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    **tokenizer_config.get_config(),\n",
    "    **token_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaTokenizerFast(name_or_path='google/gemma-2b-it', vocab_size=256000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<bos>', 'eos_token': '<eos>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<start_of_turn>', '<end_of_turn>']}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<eos>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"<bos>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t106: AddedToken(\"<start_of_turn>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t107: AddedToken(\"<end_of_turn>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing token\n",
    "* https://huggingface.co/docs/tokenizers/pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "google/gemma-2b-it\n"
     ]
    }
   ],
   "source": [
    "print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from applyllm.pipelines import (\n",
    "    ModelCatalog,\n",
    "    PromptHelper\n",
    ")\n",
    "\n",
    "model_info = ModelCatalog.get_model_info(model_name)\n",
    "prompt_helper = PromptHelper(model_info)\n",
    "\n",
    "if model_info.model_family == ModelCatalog.GOOGLE_FAMILY:\n",
    "    query = \"\"\"BEGIN EXAMPLE\n",
    "Q: Roger has 3 tennis balls. He buys 2 more cans of tennis balls. Each can has 4 tennis balls. How many tennis balls does he have now?\n",
    "A: Roger started with 3 balls. 2 cans of 4 tennis balls each is 8 tennis balls. 3 + 8 = 11. The answer is 11.\n",
    "END EXAMPLE\n",
    "\n",
    "Your turn:            \n",
    "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have? \n",
    "\"\"\"\n",
    "    inputs=[prompt_helper.gen_prompt(query)]\n",
    "else: \n",
    "    inputs=[\"\"\"\n",
    "Q: Roger has 3 tennis balls. He buys 2 more cans of tennis balls. Each can has 4 tennis balls. How many tennis balls does he have now?\n",
    "A: Roger started with 3 balls. 2 cans of 4 tennis balls each is 8 tennis balls. 3 + 8 = 11. The answer is 11.\n",
    "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
    "\"\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253\n",
      "[2, 106, 1645, 108, 2045, 708, 476, 10055, 235269, 62275, 578, 10406, 20409, 235265, 108, 24367, 3448, 685, 1707, 3948, 685, 3077, 2177, 573, 4807, 2793, 4646, 235265, 108, 6922, 10523, 1412, 1297, 3448, 573, 2872, 3631, 235269, 66004, 578, 749, 780, 791, 1089, 2793, 1452, 573, 3448, 603, 3015, 235265, 108, 6922, 10523, 1412, 1297, 614, 2793, 578, 780, 3707, 1089, 19319, 235269, 26353, 3782, 235269, 689, 1156, 93846, 235265, 109, 2495, 476, 2872, 1721, 780, 1501, 5229, 689, 603, 780, 2251, 38303, 63269, 235269, 10200, 3165, 5918, 576, 39534, 2775, 780, 5112, 235265, 1927, 692, 1453, 235303, 235251, 1230, 573, 3448, 577, 476, 2872, 235269, 3743, 1453, 235303, 235251, 4638, 1566, 2113, 235265, 6372, 1931, 590, 1453, 235303, 235251, 1230, 235265, 109, 26093, 90412, 108, 235368, 235292, 23627, 919, 235248, 235304, 22560, 20980, 235265, 1315, 58015, 235248, 235284, 978, 34252, 576, 22560, 20980, 235265, 9573, 798, 919, 235248, 235310, 22560, 20980, 235265, 2250, 1767, 22560, 20980, 1721, 693, 791, 1490, 235336, 108, 235280, 235292, 23627, 4604, 675, 235248, 235304, 20980, 235265, 235248, 235284, 34252, 576, 235248, 235310, 22560, 20980, 1853, 603, 235248, 235321, 22560, 20980, 235265, 235248, 235304, 963, 235248, 235321, 589, 235248, 235274, 235274, 235265, 714, 3448, 603, 235248, 235274, 235274, 235265, 108, 3919, 90412, 109, 6922, 2894, 235292, 149, 108, 235368, 235292, 714, 105257, 1093, 235248, 235284, 235304, 34188, 235265, 1927, 984, 1671, 235248, 235284, 235276, 577, 1501, 13955, 578, 8989, 235248, 235318, 978, 235269, 1368, 1767, 34188, 749, 984, 791, 235336, 235248, 109, 107, 108, 106, 2516, 108]\n"
     ]
    }
   ],
   "source": [
    "input_test_encoded = tokenizer.encode(inputs[0])\n",
    "print(f\"{len(input_test_encoded)}\")\n",
    "print(input_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "You are a helpful, respectful and honest assistant.\n",
      "Always answer as helpfully as possible using the context text provided.\n",
      "Your answers should only answer the question once, concise and do not have any text after the answer is done.\n",
      "Your answers should only be text and not include any HTML, bullet points, or other markup.\n",
      "\n",
      "If a question does not make sense or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. Just say I don't know.\n",
      "\n",
      "BEGIN EXAMPLE\n",
      "Q: Roger has 3 tennis balls. He buys 2 more cans of tennis balls. Each can has 4 tennis balls. How many tennis balls does he have now?\n",
      "A: Roger started with 3 balls. 2 cans of 4 tennis balls each is 8 tennis balls. 3 + 8 = 11. The answer is 11.\n",
      "END EXAMPLE\n",
      "\n",
      "Your turn:            \n",
      "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have? \n",
      "\n",
      "<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response_test_decoded = tokenizer.decode(input_test_encoded)\n",
    "print(response_test_decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'torch_dtype': torch.float16, 'device_map': 'auto', 'max_length': None, 'task': 'text-generation', 'max_new_tokens': 200, 'do_sample': True, 'temperature': 0.001, 'top_k': 3, 'top_p': 0.95, 'framework': 'pt', 'return_full_text': False, 'add_special_tokens': True}\n"
     ]
    }
   ],
   "source": [
    "# bitsandbytes quantization does not work with MPS backend\n",
    "print(pipeline_kwargs)\n",
    "\n",
    "# transformer pipeline kwargs\n",
    "tp_kwargs = {\n",
    "    \"model\": model,\n",
    "    \"tokenizer\": tokenizer,\n",
    "}\n",
    "\n",
    "tp_config = ModelConfig(model_config = tp_kwargs)\n",
    "\n",
    "generator = transformers.pipeline(\n",
    "    **tp_config.get_config(),\n",
    "    **pipeline_kwargs,\n",
    "    **token_kwargs,\n",
    "    # **compression_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Install autopep8 or black extension in VSCode\n",
    "`shift + opt + F` to auto format python code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Allocated memory : 9.961304 GB\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "from applyllm.accelerators import AcceleratorStatus\n",
    "\n",
    "gpu_status = AcceleratorStatus.create_accelerator_status()\n",
    "gpu_status.gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.6.4'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pydantic\n",
    "pydantic.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "\n",
    "\n",
    "llm = HuggingFacePipeline(\n",
    "    pipeline=generator \n",
    ")\n",
    "\n",
    "template = prompt_helper.gen_prompt(\"{input}\")\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"input\"])\n",
    "\n",
    "\n",
    "@time_func\n",
    "def chat(input) -> str:\n",
    "    \"\"\"\n",
    "    Args: \n",
    "        input: str - the input text to chat with the model, e.g. inputs[0]\n",
    "    \"\"\"\n",
    "    llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    # print(repr(llm_chain))\n",
    "    dict_response = llm_chain.invoke(input={\"input\": input})\n",
    "    return dict_response.get(\"text\", \"\")\n",
    "\n",
    "# pprint(response, indent=0, width=100)\n",
    "\n",
    "# response = chat(input=inputs[0])\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "executed: chat() python function\n",
      "walltime: 16.366790294647217 in secs.\n",
      "====================\n",
      "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
      "\n",
      "A: They had 23 apples initially. 20 apples were used to make lunch, so they had 23 - 20 = 3 apples left. 6 more apples were bought, so they had 3 + 6 = 9 apples. The answer is 9.\n"
     ]
    }
   ],
   "source": [
    "repeat = 1\n",
    "for i in range(repeat):\n",
    "    response = chat(input=inputs[0])\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "def clear_mps_memory(tokenizer, generator):\n",
    "    \"\"\"clear the MPS memory\"\"\"\n",
    "    if tokenizer is not None:\n",
    "        del tokenizer\n",
    "    if generator is not None:\n",
    "        # need to move the model to cpu before delete.\n",
    "        generator.model.cpu()\n",
    "        del generator\n",
    "    gc.collect()\n",
    "    torch.mps.empty_cache()\n",
    "    # report the GPU usage\n",
    "    gpu_status.gpu_usage()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Allocated memory : 10.228012 GB\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "gpu_status.gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs2 = [\"Which animal is the largest mammal?\"]\n",
    "# inputs2 = [\"Can you tell me something about chron's disease?\"]\n",
    "\n",
    "# hallucination https://www.findacode.com/snomed/34000006--crohns-disease.html\n",
    "\n",
    "# real answer is 34000006, probably need a RAG \n",
    "# inputs2 = [\"Which snomed ct code has chron's disease?\"]\n",
    "\n",
    "# inputs2 = [\"Can you tell me more about the company nordcloud?\"]\n",
    "inputs2 = [\"Can you tell me more about the company nordcloud in munich?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "executed: chat() python function\n",
      "walltime: 8.129618883132935 in secs.\n",
      "====================\n",
      "I am unable to provide more information about the company Nordcloud in Munich, as I do not have access to real-time information or company websites.\n"
     ]
    }
   ],
   "source": [
    "print(chat(input=inputs2[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define agent\n",
    "* https://python.langchain.com/docs/modules/agents/\n",
    "* https://python.langchain.com/docs/modules/agents/quick_start/\n",
    "* https://python.langchain.com/docs/modules/agents/how_to/custom_agent/\n",
    "\n",
    "## Gemma with custom langchain tool \n",
    "* https://github.com/Ashufet/LangChain_ReAct-Agent-with-Function-Calling_Ollama-Gemma-LLM_LangSmith\n",
    "* ReAct Agent: https://www.youtube.com/watch?v=exYUJcz4uZs\n",
    "\n",
    "## Ollama local host endpoint with LangChain\n",
    "* https://medium.com/the-constellar-digital-technology-blog/geek-out-time-play-with-langchain-2-locally-with-gemma-96c6ca370649\n",
    "\n",
    "## ReAct Agent\n",
    "* https://python.langchain.com/docs/modules/agents/agent_types/react/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model_map = {\n",
    "    \"sentence-transformers\": \"sentence-transformers/all-MiniLM-L12-v2\", # 384\n",
    "    \"baai\" : \"BAAI/bge-base-en-v1.5\" # 768 embedding dims\n",
    "}\n",
    "\n",
    "# embed_model_vendor = \"sentence-transformers\"\n",
    "embed_model_vendor = \"baai\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model_name = embed_model_map[embed_model_vendor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelConfig(model_config={'model_name': 'BAAI/bge-base-en-v1.5', 'model_kwargs': {'device': 'cpu'}, 'encode_kwargs': {'normalize_embeddings': True}})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "model_config = {\n",
    "    \"model_name\" : embed_model_name,\n",
    "    \"model_kwargs\": {'device': 'cpu'},\n",
    "    \"encode_kwargs\": {'normalize_embeddings': True}\n",
    "}\n",
    "embed_config = ModelConfig(model_config=model_config)\n",
    "\n",
    "# is downloaded at \"{MODEL_CACHE_DIR}/models/torch/sentence_transformer\" folder\n",
    "embed_model = HuggingFaceEmbeddings(\n",
    "    **embed_config.get_config()\n",
    ")\n",
    "\n",
    "embed_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from applyllm.utils import token_size\n",
    "from applyllm.pipelines import ModelConfig\n",
    "\n",
    "MAX_POSITION_EMBEDDINGS = 3072 # 4096\n",
    "CHUNK_SIZE = (MAX_POSITION_EMBEDDINGS // 1000) * 1000\n",
    "\n",
    "# Config splitter\n",
    "model_config = {\n",
    "    # Set a really small chunk size, just to show.\n",
    "    \"chunk_size\": CHUNK_SIZE,\n",
    "    \"chunk_overlap\": 200,\n",
    "    \"length_function\": token_size, # len,\n",
    "    \"is_separator_regex\": False,\n",
    "}\n",
    "\n",
    "splitter_config = ModelConfig(model_config=model_config)\n",
    "\n",
    "loader = WebBaseLoader(\"https://nordcloud.com/company/\")\n",
    "docs = loader.load()\n",
    "documents = RecursiveCharacterTextSplitter(\n",
    "    **splitter_config.get_config()\n",
    ").split_documents(docs)\n",
    "\n",
    "vector = FAISS.from_documents(documents, embed_model)\n",
    "retriever = vector.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"About Us â€“ Cloud Services â€“ Nordcloud\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nServices\\n\\nCloud Strategy\\n\\nCloudÂ\\xa0Transformation\\n\\nCloud TransformationÂ\\xa0Strategy\\nOrganisation (CCoE)Â\\xa0Design\\nCloud TransformationÂ\\xa0Consulting\\nSustainability in the Cloud\\n\\n\\nCloudÂ\\xa0Training\\n\\nGeneral CloudÂ\\xa0Courses\\nAWS CloudÂ\\xa0Courses\\nAzure CloudÂ\\xa0Training\\nGoogle CloudÂ\\xa0Courses\\nFinOpsÂ\\xa0Courses\\n\\n\\n\\n\\nCloud Foundation & Migration\\n\\nCloudÂ\\xa0Foundations\\n\\nBuild Your CloudÂ\\xa0Foundations\\nCloud Upskilling\\n\\n\\nCloudÂ\\xa0Migration\\n\\nStart your migrationÂ\\xa0journey\\nMigrate to theÂ\\xa0Cloud\\nManaged CloudÂ\\xa0Migration\\n\\n\\nCloud Security\\n\\nSecurity & Compliance\\n\\n\\n\\n\\nInnovation & Modernisation\\n\\nAI &Â\\xa0Data\\n\\nGenerative AI\\nData & AIÂ\\xa0Strategy\\nData Migration &Â\\xa0Modernisation\\nAI & Machine LearningÂ\\xa0Industrialisation\\nAI & Machine Learning at theÂ\\xa0Edge\\nBusiness Intelligence & DataÂ\\xa0Visualisation\\n\\n\\nPlatform and App modernisation\\n\\nModernise Applications to theÂ\\xa0Cloud\\nFrictionless Delivery ofÂ\\xa0Software\\n\\n\\nCloud Native Build\\n\\nApplicationÂ\\xa0Development\\nDigitalÂ\\xa0Design\\n\\n\\nCloud DevOps\\n\\nDevOps Foundation\\nKubernetes\\n\\n\\n\\n\\nManaged Services\\n\\nManaged CloudÂ\\xa0Services\\n\\nManaged CloudÂ\\xa0Foundations\\nManagedÂ\\xa0Databases\\nManaged CloudÂ\\xa0Environments\\nManaged App Modernisation\\nPES AutomationÂ\\xa0Platform\\n\\n\\nCloud PaaS Management\\n\\nApplicationÂ\\xa0Management\\nSAPÂ\\xa0OperationÂ\\xa0\\n\\n\\nFinOps\\n\\nManagedÂ\\xa0FinOps\\nFinOpsÂ\\xa0Courses\\n\\n\\n\\n\\nAll Services\\nGen AI in the cloud? Start hereAll the support you need to use gen AI and LLMs efficiently and securely ðŸ‘‡Generative AI With Public Cloud\\n\\n\\nPartners\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAmazon WebÂ\\xa0Services\\n\\nAmazon WebÂ\\xa0Services\\nAWS Services\\n\\nAWS Migration\\nAWS Data &Â\\xa0Analytics\\nAWS CloudÂ\\xa0Operations\\nAWS Managed CloudÂ\\xa0Services\\nTransforming Businesses intoÂ\\xa0SaaS\\nAWS SecurityÂ\\xa0Services\\n\\n\\nAWS Solutions\\n\\nAWS ClearStart\\nAWS End UserÂ\\xa0Computing\\nAWSÂ\\xa0Outposts\\nAWS Marketplace\\nMicrosoft onÂ\\xa0AWS\\nRunning Containers onÂ\\xa0AWS\\nVMware Cloud onÂ\\xa0AWS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMicrosoftÂ\\xa0Azure\\n\\nMicrosoftÂ\\xa0Azure\\n\\nData and AI\\nInfrastructure and migration\\nSecurity\\nManaged services\\nDigital and application innovation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGoogleÂ\\xa0Cloud\\n\\nGoogleÂ\\xa0Cloud\\nGoogle Cloud Services\\n\\nGoogle Cloud Managed CloudÂ\\xa0Services\\nMigrating to Cloud with GoogleÂ\\xa0Cloud\\nGoogle Cloud ApplicationÂ\\xa0Modernisation\\nUnlock your data with GoogleÂ\\xa0Cloud\\n\\n\\nGoogle Cloud Solutions\\n\\nGoogle Cloud VMwareÂ\\xa0Engine\\nGoogle Workspace\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSAP\\n\\nSAP\\n\\nSAP Cloud Advisory\\nSAP CloudÂ\\xa0MigrationÂ\\xa0\\nSAPÂ\\xa0OperationÂ\\xa0\\nSAPÂ\\xa0AutomationÂ\\xa0\\nSAPÂ\\xa0Innovation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIBMÂ\\xa0Multicloud\\n\\nIBMÂ\\xa0Multicloud\\n\\nIBM MulticloudÂ\\xa0Accelerator\\nIBM MulticloudÂ\\xa0Provisioning\\nIBM Multicloud Machine ImageÂ\\xa0Toolkit\\nIBM MulticloudÂ\\xa0Autobackup\\nIBM MulticloudÂ\\xa0Autopatcher\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nRed HatÂ\\xa0OpenShift\\nAll Partners\\n7 lessons from battle-hardened cloud adoptersMore details\\n\\n\\nIndustries\\n\\nAutomotive\\n\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAutomotive\\n\\nBuckle your seatbelt, and enjoy the cloud journey.\\n\\n\\nAviation\\n\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAviation\\n\\nDonâ€™t be grounded in the data centreâ€“ fly to the cloud.\\n\\n\\nFinancial Services\\n\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFinancial Services\\n\\nTick the boxes: agility, security, scalability and cost efficiency.\\n\\n\\nHealthcare\\n\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHealthcare\\n\\nDeliver cloud solutions for patients and clinicians.\\n\\n\\nManufacturing\\n\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nManufacturing\\n\\nInnovation engine that boosts efficiency and productivity.\\n\\n\\nPublicÂ\\xa0Sector\\n\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPublicÂ\\xa0Sector\\n\\nEmpowering governments to meet citizensâ€™ expectations.\\n\\n\\nServices\\n\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nServices\\n\\nImproving efficiency and reducing costs.\\n\\n\\nTechnology &Â\\xa0ISVs\\n\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTechnology &Â\\xa0ISVs\\n\\nTransform on-premises solutions to SaaS.\\n\\n\\nAll Industries\\n\\t\\t\\t\\t\\n\\nAll Industries\\n\\nTailored solutions for every industry sector.\\n\\n\\nInteractive Route Map: Aviation’s Flight PlanMore details\\n\\n\\nContentÂ\\xa0Hub\\n\\nLatest postsIT ops headaches you donâ€™t have to live withNavigating AI: How to take the next step in your cloud journey5 lesser known features of cloud marketplaces\\nTrending contentIT ops headaches you donâ€™t have to live withHow to cure Day 2 digital transformation challengesGenerative AI 101: The basics and how public cloud fits in\\nBy formatAll formatsPostsCase StudiesGuidesVideosInfographicsTech CommunityNewslettersPodcasts\\nHow to cure Day 2 digital transformation challengesGet your free guideMore details\\n\\n\\nCaseÂ\\xa0Studies\\nEvents\\nCareers\\nAboutÂ\\xa0Us\\n \\nContact Us\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nContact Us\\n\\n\\n\\n\\n\\n\\nHome > About Us\\n\\n\\n\\n\\nAbout Us.Nordcloud is a European leader in cloud implementation, application development, managed services and training. Weâ€™re triple certified and featured in Gartnerâ€™s Magic Quadrant.\\nGet in Touch \\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\nCompany Numbers. \\n\\n>45%Net revenue growth year on year\\n \\n\\n1,000+Public cloud implementations\\n \\n\\n1,300+Team members\\n \\n\\n200+Enterprise clients\\n \\n\\n10Countries with Nordcloud offices\\n \\n\\n\\n\\n\\n\\nLeadership Team. \\n\\n\\n\\n  \\nJan Kritz\\nGroup CEO\\nJanâ€™s unique combo of cloud-first expertise and positive leadership style makes him the perfect leader for Nordcloud. Together with his team of cloud ninjas, Jan supports customers at every step of their journey, helping them harness the power of an agile, cloud-native approach. Having previously held senior leadership positions at Capgemini and Atos among others, in his 5+ years at Nordcloud, heâ€™s overseen our growth as weâ€™ve cemented our position as Europeâ€™s leading cloud-native partner.\\n\\n\\n\\n\\n\\n  \\nMagnus Manders\\nHead of Service Lines\\nAfter working with Nordcloud as Group CTO at Nordea, Magnus knew he wanted to be part of our journey. With a career spanning 20 years at global giants like IKEA and Capgemini,Â\\xa0 Magnus now spends his days helping customers overcome any and all barriers to cloud adoption so they achieve both quick wins and sustainable value.\\nRead more about Magnusâ€™ experience, approach and view on cloud Â»\\n\\n\\n\\n\\n\\n  \\nTapio Koskinen\\nGlobal Sales & Marketing\\nWant a cloud-native tech whiz who truly understands things from the customerâ€™s side of the table? Tapioâ€™s your guy. As our Global Sales Lead, heâ€™s all about getting the right solutions and services to the right people, so everyone from directors to devs benefit from everything the public cloud has to offer. Certified in AWS and experienced in Azure and Google Cloud Platform, Tapio, along with his global team, give you impartial advice grounded in cloud-native experience.\\n\\n\\n\\n\\n\\n  \\nNiko MykkÃ¤nen\\nAlliances\\nWeâ€™re all about the power of collaboration at Nordcloud, which is why Niko plays such an important role. Heâ€™s the strategic link between our hyperscaler and technology partners, customers and internal teams â€“ helping everyone drive more growth and innovation from public cloud capabilities. His impressive career has spanned leadership roles at AWS and Nokia, and he has a particular interest in AI and predictive analytics.\\n\\n\\n\\n\\n\\n  \\nPetteri Uljas\\nNCEE Sales\\nPetteri began his career as a software developer and has ascended through global leadership roles and board positions across multiple countries, within Capgemini and Tietoevry. His passion lies in achieving objectives through people, forging cohesive teams from diverse professional and cultural mixes, and fostering trust that sees results for both us and our customers. Petteri believes that there’s no cloud without people.\\n\\n\\n\\n\\n\\n  \\nScott Eivers\\nUK Sales\\nScott brings a wealth of experience at building successful services businesses in the technology sector into Nordcloud. Prior to Nordcloud Scott has led a variety of growth roles including most recently building the business consulting team of Gartner in EMEA.His role today is to lead the UK region, driving growth through the multiple sales and commercial routes. This builds upon his previous role at Nordcloud as the Chief Growth Officer where he had responsibility for building various growth acceleration initiatives including marketing, product mgmt, commercial mgmt and corporate development\\n\\n\\n\\n\\n\\n  \\nThomas Baus\\nDACH Sales\\nHaving been part of Nordcloud for most of the companyâ€™s lifespan, Thomas has dedicated heart and mind to nurturing our brand, expanding our customer base, fostering our team, and enriching our culture within our German-speaking regions. After holding various global positions, he now returns to focus on our ongoing growth and success journey in the DACH region.\\n\\n\\n\\n\\n\\n  \\nNilay Mardikar\\nMigrations\\nStarting his career as a software developer, Nilay has been in the IT realm since 1994, having worked for the likes of Atos, Tietoevry, Capgemini, and Birlasoft previously. Heâ€™s committed to working closely with clients to define cloud success. Extensive experience in sales, solutions and delivery helps Nilay support individuals in various teams to thrive. In his current role, Our ambition is to broaden migration and post migration modernization delivery to foster growth, reliability, predictability with reuse and profitability for Nordcloud and IBM. Nilay has strong experience in critical delivery and sales, solutions roles including strategic engagements with different customers in Nordics, EMEA.\\n\\n\\n\\n\\n\\n  \\nIndrayudh Ghosh\\nPlatform Engineering Services\\nIndra brings extensive experience in sales and delivery across Europe, Asia, and the Americas. As a tech evangelist and growth-oriented professional, Indra specializes in orchestrating large end-to-end deliveries while maintaining a steadfast focus on people. In the constantly evolving tech landscape, Indra spearheads the expansion of Nordcloudâ€™s platform engineering services through AI and automation, driven by the collective efforts of our people.\\n\\n\\n\\n\\n\\n  \\nJaana Salmela\\nProfessional Services\\nHaving held numerous leadership roles at both global and regional levels, Jaana has played a pivotal role in shaping and enhancing Nordcloudâ€™s delivery culture and quality. With a robust background in managing enterprise-grade IT service teams and consulting organisations, she is tasked with scaling and enhancing our global professional services teams and offerings to meet the evolving demands of our key markets.\\n\\n\\n\\n\\n\\n  \\nIlja Summala\\nGroup CTO\\nIlja is a legend in the tech industry. After 12 years at tech giant Nokia, he joined Nordcloud as CTO in 2012 to lead us on our journey to become a full-stack, cloud-native system integrator. His passion and unparalleled technological skills help customers transform the way they manage infrastructure and develop applications in a cloud-native world.\\n\\n\\n\\n\\n\\n  \\nMaria Ihamuotila\\nChief of Staff\\nBringing expertise in strategy consulting, international expansion, and operational project management within dynamic, high-growth environments, Maria spearheads the development of Nordcloud’s group strategy and operations. She is driven by a passion for multidisciplinary topics and excels in translating strategy into actionable plans through clear communication, robust cross-functional collaboration, and data-driven problem-solving.\\n\\n\\n\\n\\n\\n  \\nTuomas Toropainen\\nCFO Office\\nTuomas is our numbers guy. With a finance career spanning a wide range of industries, heâ€™s on a mission to challenge traditional perceptions of finance leaders. Like Nordcloud, heâ€™s forward-thinking, and the words â€˜But thatâ€™s how weâ€™ve always done it!â€™ will never leave his mouth.\\n\\n\\n\\n\\n\\n  \\nRam Kumar\\nChief People Officer\\nCustomer value focus\\nHow do you make great things happen in the cloud? By having a crack team of cloud ninjas leading and supporting you. Which is where Ram comes in. He has the perfect combination of cloud/tech and HR experience, honed over decades working with the likes of IBM, Wipro, Nokia, Microsoft and Cloudator. His leadership helps ensure our customers and our colleagues benefit from strategies, processes and development opportunities that put them at the cutting edge of cloud.\\nTalent focus\\nOur purpose as a company is to accelerate the world to a more equal, greener and sustainable digital society. And that requires a team of uniquely talented people working together to achieve greatness. Thatâ€™s where Ram comes in. He helps us build and nurture our team of digital builders born in the cloud â€“ drawing on deep cloud/tech and people experience honed over decades working with the likes of IBM, Wipro, Nokia, Microsoft and Cloudator.Â\\xa0\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nProud to be cloud native.\\n\\n\\n\\n\\n\\n\\n\\n\\nWhy NordcloudWe help you use the cloud to become stronger, fitter and faster.\\nLearn more \\n\\nOur ApproachWe empower your business to drive value, velocity and growth with the public cloud.\\nLearn more \\n\\n\\n\\n\\n\\nCompany Timeline. \\n\\n\\n\\n2006A cloud-native infrastructure and web application development company is born in Finland!\\n2011Nordcloud was established â€“ with a focus on helping customers leverage public cloud infrastructure and DevOps. Growth skyrockets.\\nEsa Kunninen appointed as the first Nordcloud CEO.\\n2013One of our original founders, Pyry Lehdonvirta, becomes a published author with HTML5 as an Application, a go-to resource for application designers and developers â€“ cementing our position as a pioneer in emerging technologies.\\nNordcloud expands into its second country: Sweden.\\n2014Growth gets serious as Nordcloud joins the 2-year EIT Digital Accelerator to kickstart European expansion.\\nNordcloud becomes an AWS Premier Consulting partner.\\nExpansion continues, with new Nordcloud offices in Denmark and the UK.\\n2015We seize a leadership position in the cloud application design and development space by creating a living style guide, a predecessor of modern design systems.\\nNordcloud secures â‚¬1 million funding from Finnvera, the Finnish public financing company, to help drive expansion. We open new offices in Norway and Germany.\\nAs one of the worldâ€™s first AWS Lambda partners, we start delivering web-based applications based on cloud-native, serverless architectures.\\n2016Nordcloud enters the Netherlands.\\n2017Nordcloud is recognised in Gartnerâ€™s first-ever Magic Quadrant for Public Cloud Infrastructure Managed Service Providers, with Gartner particularly highlighting our multi-cloud expertise.\\nWe open our first office in Poland.\\n2018Microsoft names Nordcloud a finalist for Partner of the Year.\\nFor the second year running, Nordcloud is recognised in Gartnerâ€™s Magic Quadrant for Public Cloud Infrastructure Professional and Managed Services.\\nSwitzerland becomes the ninth country with a Nordcloud presence.\\n2019Microsoft names us Partner of the Year.\\nThe Financial Times names us as one of the 1,000 fastest-growing companies in Europe as part of its FT1000 ranking.\\nWe open an office in Austria, giving us a presence in 10 countries.\\n2020Gartner names Nordcloud the top cloud-native managed services provider in its Magic Quadrant for Public Cloud Infrastructure, Professional and Managed Services, Worldwide.\\nNordcloud achieves Google Cloud specialisations in cloud migration and training.\\nMicrosoft names us Partner of the Year.\\nIBM acquires Nordcloud, giving us new superpowers and scale for bringing cloud-native expertise to enterprise customers.\\n2021We officially launch Nordcloud Klarity, our cloud management and FinOps SaaS product. We originally developed it for our own managed services team, but based on popular demand, made it generally available for people looking to get more automation, visibility and cost control with their clouds.\\nMicrosoft names us Azure Infra Modernisation Partner of the Year.\\nAWS names us AWS Migration Partner of the Year.\\nNordcloud achieves VMware Master Services Competency in VMware Cloud on AWS, making us one of the rare partners to have such high credentials across both VMware and AWS.\\nNordcloud achieves a Google Cloud specialisation in infrastructure services.\\nGartner recognises Nordcloud as a Visionary in its Magic Quadrant for Public Cloud IT Transformation Services.\\nNordcloud becomes a signatory of Microsoftâ€™s Partner Pledge, which is a commitment to helping people use technology in the right way and for the greater good.\\nNordcloud is recognised as a Great Place to Work in Austria, Denmark, Finland, Germany, Netherlands, Poland and the UK.\\n2022Nordcloud wins a major cloud agreement with Valtori, the IT arm of the Finnish government. This cloud partnership will help the government move faster towards cloud while maintaining robust security and data protection.\\nWe hit 1000 AWS certifications and receive an AWS Partner Network (APN) Certification Distinction.\\nWe hit 1000 Microsoft certifications.\\n2023Nordcloudâ€™s founder and CEO Fernando Herrera steps down after a decade of leading the company. Jan Kritz, formerly COO, steps up to lead Nordcloud as CEO in an exciting new chapter.\\nNordcloud has been chosen by Microsoft as Western Europe Partner of the Year for Azure cloud solutions.\\n2024Nordcloud continues to drive innovation in cloud, enabling AI excellence in public cloud and revolutionising the automation of Platform Engineering Services.\\n \\n\\n\\n\\n\\nOur Offices. \\n\\n\\nAmsterdamNordcloud B.V. Netherlands\\nJohan Huizingalaan 765\\n1066 VH Amsterdam, NetherlandsShow routeBarcelonaVIEWNEXT\\nCarrer JesÃºs Serra Santamans, 4\\n08174 Sant Cugat del VallÃ¨s, BarcelonaShow routeBerlinSpaces c/o Nordcloud Germany GmbH\\nJÃ¤gerstraÃŸe 54-55\\n10117 BerlinShow routeBernNordcloud Switzerland GmbH\\nZentroom\\nBahnhofplatz 10b\\n3011 BernShow routeCopenhagenNordcloud ApS Denmark\\nAmager FÃ¦lledvej 106\\n2300 KÃ¸benhavn SShow routeGothenburgNordcloud Hosting Sweden AB\\nJohann Willins Gata 8\\n416 64 GÃ¶teborg\\n(Johan Willins Gata 6 for visits and post)Show routeHelsinkiNordcloud Oy\\nPohjoisesplanadi 37A\\n00100 HelsinkiShow routeJyvÃ¤skylÃ¤Nordcloud Oy\\nVapaudenkatu 60\\n40100 JyvÃ¤skylÃ¤Show routeJÃ¶nkÃ¶pingNordcloud Hosting Sweden AB\\nÃ–stra Storgatan 9\\n553 20 JÃ¶nkÃ¶pingShow routeKuopioNordcloud Oy\\nMicrokatu 1 (M)\\n70150 Kuopio (Novapolis)Show routeLondonIBM United Kingdom Ltd\\n20 York Road\\nLondon SE1 7NDShow routeMadridVIEWNEXT\\nAv. de Burgos, 8-A\\n28036 MadridShow routeMalmÃ¶Nordcloud Hosting Sweden AB\\nStortorget 11\\n211 22 MalmÃ¶Show routeManchesterNordcloud Ltd\\nHana, Landmark, St Peter’s Square\\nManchester M1 4BPShow routeMunichNordcloud Deutschland GmbH (IBM)\\nMies-van-der-Rohe-StraÃŸe 6\\nTower 1 / 28 OG\\n80807 MÃ¼nchenShow routeOsloNordcloud AS Norway\\nGrundingen 6\\n0250 OsloShow routeOuluNordcloud Oy\\nElektroniikkatie 13\\n90580 Oulu (Technopolis)Show routePoznaÅ„Nordcloud sp z.o.o. Poland\\nKupiec PoznaÅ„ski, 5th floor\\nPlac Wiosny LudÃ³w 2\\n61-831 PoznaÅ„Show routeSaloNordcloud Oy\\nSalo IoT Park Oy\\nJoensuunkatu 7\\n24100 SaloShow routeStockholmNordcloud Hosting Sweden AB\\nDrottninggatan 68, vÃ¥n 5\\n111 21 StockholmShow routeViennaNordcloud, Austria\\nObere DonaustraÃŸe 95\\n1020 ViennaShow routeWarsawNordcloud sp z.o.o. Poland\\nMennica Legacy Tower\\nul. Prosta 20\\n00-850 WarsawShow routeWrocÅ‚awNordcloud sp z.o.o. Poland\\nSpaces Wroclavia\\nul. Sucha 3\\n50-086 WrocÅ‚awShow routeZugNordcloud Switzerland GmbH\\nGrafenauweg 8\\n6300 ZugShow route \\n\\n\\n\\n\\n\\n\\n\\n \\n\\nGet in Touch.Letâ€™s discuss how we can help with your cloud journey. Our experts are standing by to talk about your migration, modernisation, development and skills challenges.\\n \\n\\n \\n\\nIlja Summala\\nLinkedIn\\n\\nGroup CTO\\nIljaâ€™s passion and tech knowledge help customers transform how they manage infrastructure and develop apps in cloud.\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\nSign up to our newsletter.Hear unique professional insights direct from Nordcloud's cloud native experts on the latest developments in cloud.\\r\\nWant to see what's waiting for you in the Newsletter? Get the full scoop! \\n\\n\\n\\n\\n\\nIndustries\\n\\nAutomotive\\nAviation\\nHealthcare\\nFinancial Services & Insurance\\nManufacturing & Industrial\\nPublic Sector\\nServices\\nTechnology & ISVs\\n\\n\\nPartners\\n\\nAmazon Web Services\\nMicrosoft Azure\\nGoogle Cloud\\nSAP\\nIBM Multicloud\\n\\n\\nCompany\\n\\nContact\\nCareers\\nAbout Us\\nOur Offices\\nPress Office\\nCommunity & Culture\\nInvestors\\n\\n\\nPolicies\\n\\nPrivacy Notice\\nRecruitment Privacy Policy\\nESG Policy Statement\\nQuality Policy\\nFeedback\\nSecurity\\nCookies\\nCookie Settings\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\nInstagram\\nTwitter\\nFacebook\\nLinkedin\", metadata={'source': 'https://nordcloud.com/company/', 'title': 'About Us â€“ Cloud Services â€“ Nordcloud', 'description': 'Nordcloud is a European cloud leader featured in Gartnerâ€™s Magic Quadrant. Learn how we help with Microsoft Azure, AWS and Google Cloud.', 'language': 'en-GB'})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.get_relevant_documents(inputs2[0])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"northcloud_search\",\n",
    "    \"Search for information about NorthCloud. For any questions about NorthCloud, you must use this tool!\",\n",
    ")\n",
    "\n",
    "tools = [retriever_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['agent_scratchpad', 'input', 'tool_names', 'tools'], metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'react', 'lc_hub_commit_hash': 'd15fe3c426f1c4b3f37c9198853e4a86e20c425ca7f4752ec0c9b0e97ca7ea4d'}, template='Answer the following questions as best you can. You have access to the following tools:\\n\\n{tools}\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [{tool_names}]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: {input}\\nThought:{agent_scratchpad}')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import create_react_agent\n",
    "# from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "# tools = [TavilySearchResults(max_results=1)]\n",
    "\n",
    "# Get the prompt to use - you can modify this!\n",
    "prompt = hub.pull(\"hwchase17/react\")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableAssign(mapper={\n",
       "  agent_scratchpad: RunnableLambda(lambda x: format_log_to_str(x['intermediate_steps']))\n",
       "})\n",
       "| PromptTemplate(input_variables=['agent_scratchpad', 'input'], partial_variables={'tools': 'northcloud_search: Search for information about NorthCloud. For any questions about NorthCloud, you must use this tool!', 'tool_names': 'northcloud_search'}, metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'react', 'lc_hub_commit_hash': 'd15fe3c426f1c4b3f37c9198853e4a86e20c425ca7f4752ec0c9b0e97ca7ea4d'}, template='Answer the following questions as best you can. You have access to the following tools:\\n\\n{tools}\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [{tool_names}]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: {input}\\nThought:{agent_scratchpad}')\n",
       "| RunnableBinding(bound=HuggingFacePipeline(pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x449dfc450>), kwargs={'stop': ['\\nObservation']})\n",
       "| ReActSingleInputOutputParser()"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct the ReAct agent\n",
    "agent = create_react_agent(llm, tools, prompt)\n",
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, return_intermediate_steps=True, handle_parsing_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I now think about what to do\n",
      "Action: use the [northcloud_search]\u001b[0mInvalid Format: Missing 'Action Input:' after 'Action:'\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: hi\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'hi',\n",
       " 'output': 'hi',\n",
       " 'intermediate_steps': [(AgentAction(tool='_Exception', tool_input=\"Invalid Format: Missing 'Action Input:' after 'Action:'\", log=' I now think about what to do\\nAction: use the [northcloud_search]'),\n",
       "   \"Invalid Format: Missing 'Action Input:' after 'Action:'\")]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# agent_executor.invoke({\"input\": inputs2[0]})\n",
    "agent_executor.invoke({\"input\": \"hi\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEAR_MEMORY = False\n",
    "# CLEAR_MEMORY = True\n",
    "\n",
    "if CLEAR_MEMORY:\n",
    "    clear_mps_memory(tokenizer=tokenizer, generator=generator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents3.11",
   "language": "python",
   "name": "agents3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
